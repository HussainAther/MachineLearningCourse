{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d60e50e4cc9da06e03d8e077329cf24b",
     "grade": false,
     "grade_id": "cell-342b59512d5e4070",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 8 - Classification\n",
    "\n",
    "This week's assignment will give you an opportunity to practice your classification skills with the scikit-learn package. You will have to apply several different classification methods and learn about their parameters.\n",
    "\n",
    "In sklearn the different options (called hyperparameters) for each classifier are specofied in the constructor when you initialize an object. Then, method __.fit(X, y)__ will run the training procedure where the classifier will learn from a set of samples and their features in __X__ and the corresponding class labels in __y__. Once the classifier model is trained you can use it for predicting the labels on previously unseen data with __.predict(X_new)__.\n",
    "\n",
    "You will need to use various classification metrics to __evaluate the performance of the trained model__. See http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics for more details.\n",
    "\n",
    "A balanced F1 score combining precision and recall is a good default performance measure for binary classifiers. However, for instance as in the example with handwritten digits - the problem is multiclass - where we have 10 classes, one for each digit. In extending a binary metric to multiclass or multilabel problems, it could be treated as a collection of binary problems, one for each class. It is, therefore, necessary to average the evaluation results. There is then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter. Read more about __\"micro\" and \"macro\" averaging of classification metrics__ in scikit-learn documentation.\n",
    "\n",
    "\"macro\" simply calculates the mean of the binary metrics, giving equal weight to each class. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n",
    "\n",
    "Thus if we make sure that each digit is represented by approximately equal number of samples, the performance of a classifier will be best evaluated using F1 with macro averaging across the classes. __StratifiedKfold__ class or a stratified train_test_split() can be used to make sure each class gets an approximately equal number of samples in a random split.\n",
    "\n",
    "The final, and probably, one of most important aspects of machine learning covered by this assignment - is to __avoid overfitting the models while they train__. Overfitting leads to a classifier that is showing excellent performance on the dataset used for training, while its performance on a previously unseen dataset could be average if not poor. __Cross-validation__ is an important technique to master. Fortunately, even the complex scenarios of cross-validation are already implemented and available for use in sklearn.\n",
    "\n",
    "Often we need to choose between several classification models based on their performance, for instance the classifiers initialized with different hyperparameters, we need to make sure there is no overfitting of hyperparameters. It means that the choise of hyperparameters can be optimal for the training dataset but may not be optimal in general. __GridSearchCV__ class simplifies the hyperparameter optimization procedure, but you need to make sure to put aside a subset of your data for the final validation. __train_test_split()__ is the best way to split the dataset in sklearn.\n",
    "\n",
    "There is a lot to explore in scikit learn documentation, take this opportunity, and ask your questions on Slack channel #week8 if something is unclear.\n",
    "\n",
    "Due to a heavy use of randomization techniques the exact performance mertics may not be achieved (sometimes even with the random seed set) therefore the tests in assert statements will  validate your solutions roughly. Don't be surprised to get slightly different results when you restart your calculations.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "There are 5 graded assignments and total 6 points: the first task is worth 2 points, the other four are 1 point each. Additionally, there is an ungraded assignment that we highly encourage you to solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "15ae9877e707877833a66ff936b44d1c",
     "grade": false,
     "grade_id": "cell-8cac1e0680474a0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a43862c9797e2f5fdffa26f3efa0e49",
     "grade": false,
     "grade_id": "cell-b24ccf3f1f5aad98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task #1  (2 points) - KNN\n",
    "\n",
    "Implement your own version of Nearest Neighbor classifier as a class MyKNN. The only hyperparameter to \\_\\_init\\_\\_(self, K) is K - the number of neighbors.\n",
    "\n",
    "Implement  .fit(X, y) method. There is no real training as the model simply memorizes all the data samples and their class labels.\n",
    "\n",
    "Implement .predict(X_new) method; this is where all the calculations are. You need to compare each sample in X_new to the memorized data and choose the K nearest neighbors using euclidean distance. Then you need to predict the label based on the most frequent label of the K neighbors and return an array of predicted labels. The problem could be binary or multiclass, it does not matter for the implementation, however you predict only one label for each sample, so it is not a multilabel classification problem.\n",
    "\n",
    "Do not worry about memory or speed optimization, we will not use MyKNN on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3055364cb8db4ea8fb740decc477059c",
     "grade": false,
     "grade_id": "t1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode\n",
    "\n",
    "class MyKNN():\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbor Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, K):\n",
    "        \"\"\"\n",
    "        Initialize the class MyKNN to use the Nearest Neighbor Classifier on K neighbors\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Memorize the data samples and labels\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        \"\"\"\n",
    "        Find the distance between two samples.\n",
    "        Run the training procedure where the classifier will learn from a set of samples\n",
    "        and their features in X and the corresponding class labels in y.\n",
    "        Return the most similar neighbor given a training set and test set.  \n",
    "        \"\"\"\n",
    "    \n",
    "        results = []\n",
    "        for x in X_new:\n",
    "            d = []\n",
    "            for a in range(len(self.X)):\n",
    "                d.append(euclidean(self.X[a], x))\n",
    "            d = np.array(d)\n",
    "            i = np.argpartition(d, self.K - 1,  axis=0)\n",
    "            l = self.y[i[:self.K]]\n",
    "            counts = np.bincount(l)\n",
    "            results.append(np.argmax(counts))\n",
    "        return results\n",
    "\n",
    "#         d = cdist(self.X, X_new)\n",
    "#         i = np.argpartition(d, self.K-1, axis=0)[:self.K]\n",
    "#         n = np.take(self.y, i)\n",
    "#         out = mode(n)[0][0]\n",
    "#         return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b733f2cd5dd7dfcbe7275223921ffc87",
     "grade": true,
     "grade_id": "t1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-b95210127df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.67\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mX_new2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = np.array([\n",
    "    [1.5, 2.5],\n",
    "    [3.5, 4.5],\n",
    "    [6.5, 6.5],\n",
    "    [4.5, 1.5],\n",
    "    [5.5, 3.5],\n",
    "    [7.5, 3.5]])\n",
    "\n",
    "y = np.array([0,0,1,1,2,2])\n",
    "\n",
    "X_new1 = X * 0.75\n",
    "y_new1 = y\n",
    "cls = MyKNN(4)\n",
    "cls.fit(X, y)\n",
    "pred_y = cls.predict(X_new1)\n",
    "assert round(accuracy_score(y_new1, pred_y), 2) == 0.67\n",
    "\n",
    "X_new2 = X * 1.1\n",
    "y_new2 = y\n",
    "for k in range(1, 7):\n",
    "    cls = MyKNN(k)\n",
    "    cls.fit(X, y)\n",
    "    pred_y = cls.predict(X_new2)\n",
    "    assert accuracy_score(y_new2, pred_y) == 1.0\n",
    "    assert cls.predict(np.array([[4.3, 3.6]]))[0] == 0\n",
    "    assert cls.predict(np.array([[4.2, 3.3]]))[0] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61c8c985dfd456a25a817259bfe9a51a",
     "grade": false,
     "grade_id": "cell-825e2acaf3ee4b45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2  (1 point) - Sampling\n",
    "\n",
    "Analyze stability of features in subsamples in train_test_split().\n",
    "Take the wines dataset (loaded below), and plot a distribution of \"quality\" feature. You will see that for wine quality <5 and >7 we only have a few samples; exclude these samples from wines dataset. Define __y__ as a vector of wine quality class labels. Define __X__ as the matrix of samples with the rest of the features (excluding quality).\n",
    "\n",
    "Run 1000 iterations of train_test_split() subsampling from __X__ and __y__ stratified by __y__ with 30% test size.\n",
    "\n",
    "Create __avg_alcohol_values_train__ list and append mean values of \"alcohol\" feature in the training subset on each iteration.\n",
    "\n",
    "Create __avg_alcohol_values_test__ list and append mean values of \"alcohol\" feature in the testing subset on each iteration.\n",
    "\n",
    "Analyze the mean and standard deviation of values in each of the lists. Plot histograms for visual aid.\n",
    "Think about why standard deviations of mean values from test and train samples differ and why the means are equal.\n",
    "\n",
    "The automatic tests will be checking X, y, and the avg_alcohol... lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aeba7faf9d2b3fcf7d76a1911e8c9bbf",
     "grade": false,
     "grade_id": "cell-ba7496856ec4a685",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syedather/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/Users/syedather/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "wines[\"quality\"].hist()\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c00dccbb65cf0cfffcd86ded25c8323f",
     "grade": false,
     "grade_id": "t2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvSRVzqKXpMpZ9JJUuf8PyZrB2cukfJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080436a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXpIaZ9BLUuMMeklq3P8DYejMMPFY0DwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13bb0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wines = wines[(wines.quality <= 7) & (wines.quality >= 5)]\n",
    "y = wines[\"quality\"]\n",
    "X = wines.loc[:, wines.columns != \"quality\"]\n",
    "\n",
    "avg_alcohol_values_train = []\n",
    "avg_alcohol_values_test = []\n",
    "\n",
    "for i in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "    avg_alcohol_values_train.append(np.mean(X_train[\"alcohol\"]))\n",
    "    avg_alcohol_values_test.append(np.mean(X_test[\"alcohol\"]))\n",
    "    \n",
    "plt.hist(avg_alcohol_values_train)\n",
    "plt.show()\n",
    "plt.hist(avg_alcohol_values_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b883448456eb3581daaea89ac595eea",
     "grade": true,
     "grade_id": "t2-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "\n",
    "assert 0.034 <= round(pd.Series(avg_alcohol_values_test).std(), 3) <= 0.037\n",
    "assert round(pd.Series(avg_alcohol_values_test).mean(), 1) == round(pd.Series(avg_alcohol_values_train).mean(), 1)\n",
    "\n",
    "assert 0.014 <= round(pd.Series(avg_alcohol_values_train).std(), 3) <= 0.016\n",
    "assert round(pd.Series(avg_alcohol_values_train).mean(), 1) == round(wines['alcohol'].mean(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5560d561a5f36a659ef364ebb63f6726",
     "grade": false,
     "grade_id": "cell-6e46df97f5e597d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3 (1 point) - Multinomial NB\n",
    "\n",
    "Use RepeatedStratifiedKfold cross-validation to evaluate performance of a Multinomial Naive Bayes on the wine dataset predicting wine quality. As in the previous task exclude wines with quality that is not 5,6 or 7; and prepare sample-feature matrix X and vector with class labels y accordingly.\n",
    "\n",
    "Use the following classesfrom sklearn:  RepeatedStratifiedKFold, MultinomialNB and cross_val_score function.\n",
    "\n",
    "Initialize Repeated 5-fold cross validation object (5 splits) with 1000 repetitions.\n",
    "Initialize Multinomial NB classifier.\n",
    "Calculate cross validation scores for the NB classifier and the kfold cross validation object that we created (use cv parameter to pass a custom cross validation object to cros_val_score function). For scoring use F1 score with macro averageing 'f1_macro'. If you want to speed things up set n_jobs=-1  which would use the power of multiple CPU cores on your computer for the calculation.\n",
    "\n",
    "Store the results of cross validation scores in \"scores\" variable. We will check it in assert tests, as well as X and y will be tested.\n",
    "\n",
    "For visual aid plot a distribution of scores across cross validation runs. Now you have a robust estimate of Multinomial NB classifier performance in predicting wine quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ad3fb1c62caac05fc258f474d9b8d04",
     "grade": false,
     "grade_id": "cell-f150c53545a8aa82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "bfe37efa32c2bab16b0bb20c4c6791c9",
     "grade": false,
     "grade_id": "cell-762846b2ba96f881",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "wines = wines[(wines.quality <= 7) & (wines.quality >= 5)]\n",
    "y = wines[\"quality\"]\n",
    "X = wines.loc[:, wines.columns != \"quality\"]\n",
    "\n",
    "for i in range(1000):\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1000)\n",
    "    \n",
    "clf = MultinomialNB()\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "589995734ac5e8c080c37119bccf94b6",
     "grade": true,
     "grade_id": "cell-666c6eb3182f9908",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "assert  .4 < pd.Series(scores).mean() < .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d0ae26be5fdad3ed37a7e685bd9a671",
     "grade": false,
     "grade_id": "cell-3534396446b0c46b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 4 (1 point) - Logistic Regression\n",
    "\n",
    "Use RepeatedStratifiedKfold cross-validation to evaluate performance of a Logistic Regression on the wine dataset predicting wine quality. \n",
    "\n",
    "This time let us turn it into a binary classification problem and prepare a vector with class labels y that will only have 1 (for high-quality wine where quality >= 7) and 0 for the rest of wine samples.\n",
    "Initialize sample-feature matrix X (don't forget to exclude quality column).\n",
    "\n",
    "Use the following classesfrom sklearn:  RepeatedStratifiedKFold, LogisticRegression and cross_val_score function.\n",
    "\n",
    "As in the previous example: \n",
    "\n",
    "Initialize Repeated 5-fold cross validation object (5 splits) with 1000 repetitions.\n",
    "Initialize Multinomial NB classifier.\n",
    "Calculate cross validation scores for the NB classifier and the kfold cross validation object that we created (use cv parameter to pass a custom cross validation object to cros_val_score function).\n",
    "\n",
    "However, for scoring let us use accuracy this time, and since it is a binary classification problem no averageing across classes is required. If you want to speed things up set n_jobs=-1  which would use the power of multiple CPU cores on your computer for the calculation.\n",
    "\n",
    "Store the results of cross validation scores in \"scores\" variable. We will check it in assert tests, as well as X and y will be tested.\n",
    "\n",
    "For visual aid plot a distribution of scores across cross validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "829c31f4a67944462d7d9a1baea8b995",
     "grade": false,
     "grade_id": "cell-69bbd166ac4a50a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "368dcd76be58e9af7c2b673198df4be5",
     "grade": false,
     "grade_id": "cell-d707c326d2bdb279",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.where(wines[\"quality\"]>=7, 1, 0)\n",
    "X = wines.loc[:, wines.columns != \"quality\"]\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1000)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(lr, X, y, cv=rskf, scoring='accuracy')\n",
    "\n",
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db70f851842a179e269727c9d897189e",
     "grade": true,
     "grade_id": "cell-42b2feb509ff1a9d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1599,)\n",
    "assert X.shape == (1599, 11)\n",
    "assert  pd.Series(scores).mean() > 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0dc0774f1a40ade484032213fd8dd434",
     "grade": false,
     "grade_id": "cell-e6886c0f1a409bf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 5 (1 point) - Random Forest\n",
    "\n",
    "Let us use the digits dataset and apply Random Forest to predict the digit given a low resolution handwritten image.\n",
    "\n",
    "Use the following classes from sklearn:  RandomForestClassifier\n",
    "\n",
    "You will need to split data into training and 30% testing. Use training to fit RandomForestClassifier. And use testing subset to test the f1 score with macro averaging: first, predict the labels for test set using .predict(), then compare predicted to actual labels and calculate the score. Store this score in \"score\" variable, we will test it.\n",
    "\n",
    "Now let us compare the results to a robust estimate of performance using 5-fold cross validation (5 splits) with 100 repetitions (more repetitions will take time). Calculate 'f1_macro'  scores with cross validation for the whole dataset of X and y (not the training or testing subsets). Store the results of cross validation scores in \"scores\" variable (it will be tested). Let us now take the mean of \"scores\" and compare it to \"score\" that we calculated previously. What do you see?\n",
    "For visual aid plot distributions of scores across cross validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cb35028af3d86e0921d48774c2038544",
     "grade": false,
     "grade_id": "cell-f01ff137236eed5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# X - how digits are handwritten\n",
    "X = digits['data']\n",
    "\n",
    "# y - what these digits actually are\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "76b3aa740f34f592e69998775ea50d1f",
     "grade": false,
     "grade_id": "cell-9b650ceb67fbab9b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=123456)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy Score \" + str(score))\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=100)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(lr, X, y, cv=rskf, scoring='f1_macro')\n",
    "\n",
    "print(\"Mean of cross-validated score \" + str(np.mean(scores)))\n",
    "\n",
    "plt.hist(scores)\n",
    "plt.show()\n",
    "# Our cross-validated score is higher than the accuracy score. \n",
    "# From the plot of scores, we see a mean cross-validated score near the upper end of the \n",
    "# distribution. The entire distribution takes a normal Gaussian curve centered around approximately 0.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4c3121bb1463952c522585b84b9853a8",
     "grade": true,
     "grade_id": "cell-64444146993f782e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert X.shape == (1797, 64)\n",
    "assert y.shape == (1797, )\n",
    "assert score > 0.91\n",
    "assert np.percentile(scores, 5) < score < np.percentile(scores, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ecaffc8e4c2ebf10b1488c06755b4c9b",
     "grade": false,
     "grade_id": "cell-39b7df583ce9ce92",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 6 (not graded) - Grid SearchCV\n",
    "\n",
    "Apply KNN classifier to digits dataset. You need to find the optimal parameters for KNN.\n",
    "Number of neighbors is not the only parameter that can be changed. As you can see from the first exercise the distance metric is an important parameter. Also, when choosing the class label by voting between K nearest neigbors different weights could be assigned to samples. A popular choice would be to assign higher weights to neighbors that are closer.\n",
    "\n",
    "Split the dataset into train_test and validation and only use the train_test portion for grid search CV.\n",
    "Use Grid Search cross validation to find the optimal combination of parameters listed below:\n",
    "\n",
    "n_neighbors:  3, 5, 7, 9, 11, 13, 15\n",
    "weights: uniform, distance\n",
    "metric: euclidean, manhattan, chebyshev\n",
    "\n",
    "In order to define what is an optimal solution you need to choose a score. The choice is up to you, but remember that digits dataset involves multiple classes. There is no assert test.\n",
    "\n",
    "You will need grid.best_params_ and grid.best_estimator_\n",
    "\n",
    "Use the best performing classifier grid.best_estimator_ to predict values for the validation portion of your dataset. Print a confusion matrix (use confusion_matrix function) and analyze which digits are easily confused with the other ones by the classifier.\n",
    "\n",
    "If you have questions about the solution mark them clearly and we will answer in okpy grading system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c73eec3a43b99f9c3ed4ab1553b19f3",
     "grade": false,
     "grade_id": "cell-ccb9196a5de0a293",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# X - how digits are handwritten\n",
    "X = digits['data']\n",
    "\n",
    "# y - what these digits actually are\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "29650af10f742a016012324a3e8cffa5",
     "grade": true,
     "grade_id": "cell-22e9ce2d35f7301f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "clf.fit(X, y)\n",
    "print(y)\n",
    "pred_y = clf.predict(X)\n",
    "print(pred_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
