{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression\n",
    "\n",
    "Syed Hussain Ather\n",
    "\n",
    "(from https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "## Overview:\n",
    "+ Ridge Regression:\n",
    "+ + Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "+ + Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "+ Lasso Regression:\n",
    "+ + Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "+ + Minimization objective = LS Obj + α * (sum of absolute value of coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x111166e80>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs4AAAJCCAYAAADQnkGPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABoIZwAAaCCcAQCggXAGAIAG/xeoO4kxXe+fdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108315f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "\n",
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "np.random.seed(10)  #Setting seed for reproducibility\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "plt.plot(data['x'],data['y'],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resembles a sine curve but not exactly because of the noise. We’ll use this as an example to test different scenarios in this article. Lets try to estimate the sine function using polynomial regression with powers of x form 1 to 15. Lets add a column for each power upto 15 in our dataframe. This can be accomplished using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2,16):  #power of 1 is already there\n",
    "    colname = 'x_%d'%i      #new var will be x_power\n",
    "    data[colname] = data['x']**i\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the 15 powers, lets make 15 different linear regression models with each model containing variables with powers of x from 1 to the particular model number. For example, the feature set of model 8 will be – {x, x_2, x_3, … ,x_8}.\n",
    "\n",
    "First, we’ll define a generic function which takes in the required maximum power of x as an input and returns a list containing – [ model RSS, intercept, coef_x, coef_x2, … upto entered power ]. Here RSS refers to ‘Residual Sum of Squares’ which is nothing but the sum of square of errors between the predicted and actual values in the training data set. The python code defining the function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Linear Regression model from scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    #initialize predictors:\n",
    "    predictors=['x']\n",
    "    if power>=2:\n",
    "        predictors.extend(['x_%d'%i for i in range(2,power+1)])\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression(normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for power: %d'%power)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([linreg.intercept_])\n",
    "    ret.extend(linreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this function will not plot the model fit for all the powers but will return the RSS and coefficients for all the models. I’ll skip the details of the code for now to maintain brevity. I’ll be happy to discuss the same through comments below if required.\n",
    "\n",
    "Now, we can make all 15 models and compare the results. For ease of analysis, we’ll store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend. Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the powers for which a plot is required:\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "#Iterate through all powers and assimilate results\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect the models with increasing complexity to better fit the data and result in lower RSS values. This can be verified by looking at the plots generated for 6 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly aligns with our initial understanding. As the model complexity increases, the models tends to fit even smaller deviations in the training data set. Though this leads to overfitting, lets keep this issue aside for some time and come to our main objective, i.e. the impact on the magnitude of coefficients. This can be analysed by looking at the data frame created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly evident that the size of coefficients increase exponentially with increase in model complexity. I hope this gives some intuition into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.\n",
    "\n",
    "Lets try to understand this even better.\n",
    "\n",
    "What does a large coefficient signify? It means that we’re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome. When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data.\n",
    "\n",
    "I hope the concept is clear. I’ll be happy to discuss further in comments if needed. Now, lets understand ridge and lasso regression in detail and see how well they work for the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
    "\n",
    "Objective = RSS + α * (sum of square of coefficients)\n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "+ α = 0:\n",
    "+ + The objective becomes same as simple linear regression. \n",
    "+ + We’ll get the same coefficients as simple linear regression.\n",
    "+ α = ∞:\n",
    "+ + The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "+ 0 < α < ∞:\n",
    "+ + The magnitude of α will decide the weightage given to different parts of objective.\n",
    "+ + The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "\n",
    "I hope this gives some sense on how α would impact the magnitude of coefficients. One thing is for sure that any non-zero value would give values less than that of simple linear regression. By how much? We’ll find out soon. Leaving the mathematical details for later, lets see ridge regression in action on the same problem as above.\n",
    "\n",
    "First, lets define a generic function for ridge regression similar to the one defined for simple linear regression. The Python code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ‘Ridge’ function used here. It takes ‘alpha’ as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well.\n",
    "\n",
    "Now, lets analyze the result of Ridge regression for 10 different values of α ranging from 1e-15 to 20. These values have been chosen so that we can easily analyze the trend with change in values of α. These would however differ from case to case.\n",
    "\n",
    "Note that each of these 10 models will contain all the 15 variables and only the value of alpha would differ. This is different from the simple linear regression case where each model had a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize predictors to be set of 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (eg. alpha = 5). Thus alpha should be chosen wisely. A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.\n",
    "\n",
    "Lets have a look at the value of coefficients in the above models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This straight away gives us the following inferences:\n",
    "\n",
    "1. The RSS increases with increase in alpha, this model complexity reduces\n",
    "2. An alpha as small as 1e-15 gives us significant reduction in magnitude of coefficients. How? Compare the coefficients in the first row of this table to the last row of simple linear regression table.\n",
    "3. High alpha values can lead to significant underfitting. Note the rapid increase in RSS for values of alpha greater than 1\n",
    "4. Though the coefficients are very very small, they are NOT zero.\n",
    "\n",
    "The first 3 are very intuitive. But #4 is also a crucial observation. Let’s reconfirm the same by determining the number of zeros in each row of the coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that all the 15 coefficients are greater than zero in magnitude (can be +ve or -ve). Remember this observation and have a look again until its clear. This will play an important role in later while comparing ridge with lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesn’t give much of an idea but there are 2 key words here – ‘absolute‘ and ‘selection‘.\n",
    "\n",
    "Lets consider the former first and worry about the latter later.\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
    "\n",
    "Objective = RSS + α * (sum of absolute value of coefficients)\n",
    "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
    "\n",
    "1. α = 0: Same coefficients as simple linear regression\n",
    "2. α = ∞: All coefficients zero (same logic as before)\n",
    "3. 0 < α < ∞: coefficients between 0 and that of simple linear regression\n",
    "\n",
    "Yes its appearing to be very similar to Ridge till now. But just hang on with me and you’ll know the difference by the time we finish. Like before, lets run lasso regression on the same problem as above. \n",
    "\n",
    "First we’ll define a generic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional parameters defined in Lasso function – ‘max_iter‘. This is the maximum number of iterations for which we want the model to run if it doesn’t converge before. This exists for Ridge as as well but setting this to a higher than default value was required in this case. Why? I’ll come to this in next section, just keep it in the back of the envelope.\n",
    "\n",
    "Lets check the output for 10 different values of alpha using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize predictors to all 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again tells us that the model complexity decreases with increase in the values of alpha. But notice the straight line at alpha=1. Appears a bit strange to me. Let’s explore this further by looking at the coefficients.\n",
    "\n",
    "Apart from the expected inference of higher RSS for higher alphas, we can see the following:\n",
    "\n",
    "1. For the same values of alpha, the coefficients of lasso regression are much smaller as compared to that of ridge regression (compare row 1 of the 2 tables).\n",
    "2. For the same alpha, lasso has higher RSS (poorer fit) as compared to ridge regression\n",
    "3. Many of the coefficients are zero even for very small values of alpha\n",
    "\n",
    "Inferences #1,2 might not generalize always but will hold for many cases. The real difference from ridge is coming out in the last inference. Lets check the number of coefficients which are zero in each model using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that even for a small value of alpha, a significant number of coefficients are zero. This also explains the horizontal line fit for alpha=1 in the lasso plots, its just a baseline model! This phenomenon of most of the coefficients being zero is called ‘sparsity‘. Although lasso performs feature selection, this level of sparsity is achieved in special cases only which we’ll discuss towards the end.\n",
    "\n",
    "This has some really interesting implications on the use cases of lasso regression as compared to that of ridge regression. But before coming to the final comparison, lets take a bird’s eye view of the mathematics behind why coefficients are zero in case of lasso but not ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
